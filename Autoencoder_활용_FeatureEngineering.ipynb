{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EITJvSPBM4aO"
   },
   "source": [
    "# 구글 마운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcY2NsQiTfAN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UreujEpXMxIc"
   },
   "source": [
    "# **라이브러리 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQPHkLzjMxIh"
   },
   "outputs": [],
   "source": [
    "! pip install kaggler\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "from kaggler.model import AutoLGB\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3Xms5aZMxIi"
   },
   "outputs": [],
   "source": [
    "print('현재경로: {}'.format(os.getcwd()))\n",
    "\n",
    "# index는 무의미한 열이며, FLAGE_MOBIL은 모두 같은 값을 지니기에 삭제\n",
    "train = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/train.csv\")\n",
    "test = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/test.csv\")\n",
    "submission = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/sample_submission.csv\")\n",
    "\n",
    "\n",
    "train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
    "test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
    "\n",
    "train_original = train.copy()\n",
    "train_original2 = train.copy()\n",
    "test_original = test.copy()\n",
    "test_original2 = test.copy()\n",
    "train_original3 = train.copy()\n",
    "test_original3 = test.copy()\n",
    "\n",
    "print('train의 Shape: {}'.format(train.shape))\n",
    "print('test의 Shape: {}'.format(test.shape))\n",
    "print('submission의 Shape: {}'.format(submission.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uHoBFmsakkf"
   },
   "source": [
    "## FIT 객체 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eov38OoRT9fN"
   },
   "outputs": [],
   "source": [
    "def make_fit_instance(train):\n",
    "\n",
    "\n",
    "    # 결측치 처리\n",
    "    train.loc[train['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    train['occyp_type'] = train['occyp_type'].fillna('None')\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 직업별 income_total의 평균을 도출한 후 직업 소득 세분화 상위 50%이하는 0, 50~25퍼는 1, 25퍼 이하는 2으로 직업별 군집화 \n",
    "    train['income_total_group'] = 999 # 초기화\n",
    "    income_total_groupby = train.groupby('occyp_type').agg('mean')[['income_total']].sort_values(by='income_total') # 직업별 소득 평균에 대한 순위 산출\n",
    "    poor_job = list(income_total_groupby.loc[income_total_groupby['income_total'] <= train.income_total.quantile(0.5)].index)\n",
    "    ordinary_job = list(income_total_groupby.loc[(income_total_groupby['income_total'] > train.income_total.quantile(0.5)) & (income_total_groupby['income_total'] <= train.income_total.quantile(0.75))].index)\n",
    "    rich_job = list(income_total_groupby.loc[income_total_groupby['income_total'] > train.income_total.quantile(0.75)].index)\n",
    "\n",
    "    train.loc[train['occyp_type'].isin(poor_job), 'income_total_group'] = 0\n",
    "    train.loc[train['occyp_type'].isin(ordinary_job), 'income_total_group'] = 1\n",
    "    train.loc[train['occyp_type'].isin(rich_job), 'income_total_group'] = 2\n",
    "\n",
    "    train.drop('occyp_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 자동차와 집은 고가 재산 --> 두개 모두 소유 vs 한개만소유 vs 아예 없는 유형 유의미할 듯?\n",
    "    train['gender'] = train['gender'].replace(['F','M'], [0,  1])\n",
    "    train['car'] = train['car'].replace(['N', 'Y'], [0, 1])\n",
    "    train['reality'] = train['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    train['car_reality'] = train['car'] + train['reality']\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 나이변수 구간화 --> 20 ~ 69세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_BIRTH'] = train['DAYS_BIRTH'] * -1\n",
    "    train['DAYS_BIRTH_bin'] = 9999\n",
    "    train.loc[(365*20 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    train.loc[(365*30 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    train.loc[(365*40 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    train.loc[(365*50 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    train.loc[(365*60 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "\n",
    "\n",
    "    # 4. 아이들의 수: 없음 // 1~2명 // 3명이상으로 구분 \n",
    "    train['child_num_group'] = 99\n",
    "    train.loc[train['child_num'] == 0, 'child_num_group'] = 0\n",
    "    train.loc[train['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    train.loc[train['child_num'] > 2, 'child_num_group'] = 2\n",
    "    train.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 가족 사이즈 1 // 2~4 // 5~ 구분\n",
    "    train['family_size_group'] = 99\n",
    "    train.loc[train['family_size'] == 1, 'family_size_group'] = 0\n",
    "    train.loc[train['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    train.loc[train['family_size'] > 4, 'family_size_group'] = 2\n",
    "    train.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 6. 결혼 // 혼자사는사람 // 결혼을 했으나 사정상 혼자사는 사람 0,1,2 구분\n",
    "    train['family_type_group'] = 999\n",
    "    train.loc[train['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    train.loc[train['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    train.loc[train['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    train.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 7. edu_type 학력순으로 label-encoding \n",
    "    train['edu_type_labelencoding'] = 999\n",
    "    train.loc[train['edu_type'] == 'Academic degree', 'edu_type_labelencoding'] = 4\n",
    "    train.loc[train['edu_type'] == 'Higher education', 'edu_type_labelencoding'] = 3\n",
    "    train.loc[train['edu_type'] == 'Incomplete higher', 'edu_type_labelencoding'] = 2\n",
    "    train.loc[train['edu_type'] == 'Secondary / secondary special', 'edu_type_labelencoding'] = 1\n",
    "    train.loc[train['edu_type'] == 'Lower secondary', 'edu_type_labelencoding'] = 0\n",
    "    train.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 8. 근로변수 구간화-> 20 ~ 40세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'] * -1\n",
    "    train['DAYS_EMPLOYED_bin'] = 9999\n",
    "    train.loc[ ( (train['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    train.loc[(0 < train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    train.loc[(365*5 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    train.loc[(365*10 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    train.loc[(365*20 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    train.loc[(365*30 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    train.loc[(365*40 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    train.loc[(365*50 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    train.loc[(365*60 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    train.loc[(365*70 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "\n",
    "\n",
    "    # 9. 근로 일수에 따른 수입 (연간 소득을 년차 평준화해주는느낌..)\n",
    "    train['EMPLOYED_INCOME'] = 9999\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    train['EMPLOYED_INCOME'] = train['EMPLOYED_INCOME'] * train['income_total']\n",
    "\n",
    "\n",
    "\n",
    "    # FIT value_counts() 변수\n",
    "    dict_income_type_valuecount = train['income_type'].value_counts().to_dict()\n",
    "    dict_house_type_valuecount = train['house_type'].value_counts().to_dict()\n",
    "    train['income_type_count'] = train['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    train['house_type_count'] = train['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # max, mean, min\n",
    "    ### DAYS_BIRTH_bin\n",
    "    dict_DAYS_BIRTH_bin_mean = train.groupby('DAYS_BIRTH_bin').agg('mean')['income_total'].to_dict()\n",
    "    dict_DAYS_BIRTH_bin_max = train.groupby('DAYS_BIRTH_bin').agg('max')['income_total'].to_dict()\n",
    "    dict_DAYS_BIRTH_bin_min = train.groupby('DAYS_BIRTH_bin').agg('min')['income_total'].to_dict()\n",
    "    train['averageincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    train['maxincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    train['minincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "\n",
    "    ### DAYS_EMPLOYED_bin\n",
    "    dict_DAYS_EMPLOYED_bin_mean = train.groupby('DAYS_EMPLOYED_bin').agg('mean')['income_total'].to_dict()\n",
    "    dict_DAYS_EMPLOYED_bin_max = train.groupby('DAYS_EMPLOYED_bin').agg('max')['income_total'].to_dict()\n",
    "    dict_DAYS_EMPLOYED_bin_min = train.groupby('DAYS_EMPLOYED_bin').agg('min')['income_total'].to_dict()\n",
    "    train['averagehouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    train['maxinhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    train['mininhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "\n",
    "    ### house_type\n",
    "    dict_house_type_mean = train.groupby('house_type').agg('mean')['income_total'].to_dict()\n",
    "    dict_house_type_max = train.groupby('house_type').agg('max')['income_total'].to_dict()\n",
    "    dict_house_type_min = train.groupby('house_type').agg('min')['income_total'].to_dict()\n",
    "    train['averagerealhouse'] = train['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    train['maxrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    train['minrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "\n",
    "    ### edu_type_labelencoding\n",
    "    dict_edu_type_labelencoding_mean = train.groupby('edu_type_labelencoding').agg('mean')['income_total'].to_dict()\n",
    "    dict_edu_type_labelencoding_max = train.groupby('edu_type_labelencoding').agg('max')['income_total'].to_dict()\n",
    "    dict_edu_type_labelencoding_min = train.groupby('edu_type_labelencoding').agg('min')['income_total'].to_dict()\n",
    "    train['averageedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    train['maxedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    train['minedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    # FIT onehotencoder\n",
    "    OH_encoder1 = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_encoder1.fit_transform(train[['income_type']])\n",
    "    OH_cols_train1 = pd.DataFrame(OH_encoder1.fit_transform(train[['income_type']]), index=train.index, columns = list(OH_encoder1.get_feature_names()))\n",
    "    train.drop('income_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train1], axis=1)\n",
    "\n",
    "    OH_encoder2 = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_encoder2.fit_transform(train[['house_type']])\n",
    "    OH_cols_train2 = pd.DataFrame(OH_encoder2.fit_transform(train[['house_type']]), index=train.index, columns = list(OH_encoder2.get_feature_names()))\n",
    "    train.drop('house_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # binary sum 열 생성\n",
    "    binary = ['gender','car','reality','work_phone','phone','email']\n",
    "    train['bin_sum'] = train[binary].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # FIT scaler\n",
    "    standardscaler = StandardScaler()\n",
    "    train['income_stand'] = standardscaler.fit_transform(train[['income_total']])\n",
    "\n",
    "    minmaxscaler = MinMaxScaler()\n",
    "    train['income_minmax'] = minmaxscaler.fit_transform(train[['income_total']])\n",
    "\n",
    "\n",
    "\n",
    "    # KMEAN\n",
    "    train_x = train.drop([\"credit\"], axis = 1)\n",
    "    kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n",
    "    kmeans.fit(train_x)\n",
    "\n",
    "\n",
    "\n",
    "    return dict_income_type_valuecount, dict_house_type_valuecount, dict_DAYS_BIRTH_bin_mean, dict_DAYS_BIRTH_bin_max, dict_DAYS_BIRTH_bin_min, dict_DAYS_EMPLOYED_bin_mean, dict_DAYS_EMPLOYED_bin_max, dict_DAYS_EMPLOYED_bin_min, dict_house_type_mean, dict_house_type_max, dict_house_type_min, dict_edu_type_labelencoding_mean, dict_edu_type_labelencoding_max, dict_edu_type_labelencoding_min, OH_encoder1, OH_encoder2, standardscaler, minmaxscaler, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77lnH33HZqYf"
   },
   "outputs": [],
   "source": [
    "dict_income_type_valuecount, dict_house_type_valuecount, dict_DAYS_BIRTH_bin_mean, dict_DAYS_BIRTH_bin_max, dict_DAYS_BIRTH_bin_min, dict_DAYS_EMPLOYED_bin_mean, dict_DAYS_EMPLOYED_bin_max, dict_DAYS_EMPLOYED_bin_min, dict_house_type_mean, dict_house_type_max, dict_house_type_min, dict_edu_type_labelencoding_mean, dict_edu_type_labelencoding_max, dict_edu_type_labelencoding_min, OH_encoder1, OH_encoder2, standardscaler, minmaxscaler, kmeans = make_fit_instance(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgzbXnbEXFhY"
   },
   "source": [
    "## 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R4cTJYXXHAh"
   },
   "outputs": [],
   "source": [
    "def preprocessing(train, test, mode):\n",
    "\n",
    "\n",
    "\n",
    "    # 결측치 처리\n",
    "    train.loc[train['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    train['occyp_type'] = train['occyp_type'].fillna('None')\n",
    "    test.loc[test['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    test['occyp_type'] = test['occyp_type'].fillna('None')\n",
    "\n",
    "\n",
    "\n",
    "    # 1. 직업별 income_total의 평균을 도출한 후 직업 소득 세분화 상위 50%이하는 0, 50~25퍼는 1, 25퍼 이하는 2으로 직업별 군집화 \n",
    "    train['income_total_group'] = 999 # 초기화\n",
    "    income_total_groupby = train.groupby('occyp_type').agg('mean')[['income_total']].sort_values(by='income_total') # 직업별 소득 평균에 대한 순위 산출\n",
    "    poor_job = list(income_total_groupby.loc[income_total_groupby['income_total'] <= train.income_total.quantile(0.5)].index)\n",
    "    ordinary_job = list(income_total_groupby.loc[(income_total_groupby['income_total'] > train.income_total.quantile(0.5)) & (income_total_groupby['income_total'] <= train.income_total.quantile(0.75))].index)\n",
    "    rich_job = list(income_total_groupby.loc[income_total_groupby['income_total'] > train.income_total.quantile(0.75)].index)\n",
    "\n",
    "    train.loc[train['occyp_type'].isin(poor_job), 'income_total_group'] = 0\n",
    "    train.loc[train['occyp_type'].isin(ordinary_job), 'income_total_group'] = 1\n",
    "    train.loc[train['occyp_type'].isin(rich_job), 'income_total_group'] = 2\n",
    "\n",
    "    test.loc[test['occyp_type'].isin(poor_job), 'income_total_group'] = 0\n",
    "    test.loc[test['occyp_type'].isin(ordinary_job), 'income_total_group'] = 1\n",
    "    test.loc[test['occyp_type'].isin(rich_job), 'income_total_group'] = 2\n",
    "\n",
    "    test.drop('occyp_type', axis=1, inplace=True)\n",
    "    train.drop('occyp_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 자동차와 집은 고가 재산 --> 두개 모두 소유 vs 한개만소유 vs 아예 없는 유형 유의미할 듯?\n",
    "    train['gender'] = train['gender'].replace(['F','M'], [0,  1])\n",
    "    train['car'] = train['car'].replace(['N', 'Y'], [0, 1])\n",
    "    train['reality'] = train['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    train['car_reality'] = train['car'] + train['reality']\n",
    "\n",
    "    test['gender'] = test['gender'].replace(['F','M'], [0,  1])\n",
    "    test['car'] = test['car'].replace(['N', 'Y'], [0, 1])\n",
    "    test['reality'] = test['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    test['car_reality'] = test['car'] + test['reality']\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 나이변수 구간화 --> 20 ~ 69세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_BIRTH'] = train['DAYS_BIRTH'] * -1\n",
    "    train['DAYS_BIRTH_bin'] = 9999\n",
    "    train.loc[(365*20 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    train.loc[(365*30 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    train.loc[(365*40 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    train.loc[(365*50 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    train.loc[(365*60 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "    test['DAYS_BIRTH'] = test['DAYS_BIRTH'] * -1\n",
    "    test['DAYS_BIRTH_bin'] = 9999\n",
    "    test.loc[(365*20 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    test.loc[(365*30 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    test.loc[(365*40 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    test.loc[(365*50 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    test.loc[(365*60 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "\n",
    "\n",
    "    # 4. 아이들의 수: 없음 // 1~2명 // 3명이상으로 구분 \n",
    "    train['child_num_group'] = 99\n",
    "    train.loc[train['child_num'] == 0, 'child_num_group'] = 0\n",
    "    train.loc[train['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    train.loc[train['child_num'] > 2, 'child_num_group'] = 2\n",
    "    train.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "    test['child_num_group'] = 99\n",
    "    test.loc[test['child_num'] == 0, 'child_num_group'] = 0\n",
    "    test.loc[test['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    test.loc[test['child_num'] > 2, 'child_num_group'] = 2\n",
    "    test.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 가족 사이즈 1 // 2~4 // 5~ 구분\n",
    "    train['family_size_group'] = 99\n",
    "    train.loc[train['family_size'] == 1, 'family_size_group'] = 0\n",
    "    train.loc[train['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    train.loc[train['family_size'] > 4, 'family_size_group'] = 2\n",
    "    train.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "    test['family_size_group'] = 99\n",
    "    test.loc[test['family_size'] == 1, 'family_size_group'] = 0\n",
    "    test.loc[test['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    test.loc[test['family_size'] > 4, 'family_size_group'] = 2\n",
    "    test.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 6. 결혼 // 혼자사는사람 // 결혼을 했으나 사정상 혼자사는 사람 0,1,2 구분\n",
    "    train['family_type_group'] = 999\n",
    "    train.loc[train['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    train.loc[train['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    train.loc[train['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    train.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "    test['family_type_group'] = 999\n",
    "    test.loc[test['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    test.loc[test['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    test.loc[test['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    test.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 7. edu_type 학력순으로 label-encoding \n",
    "    train['edu_type_labelencoding'] = 999\n",
    "    train.loc[train['edu_type'] == 'Academic degree', 'edu_type_labelencoding'] = 4\n",
    "    train.loc[train['edu_type'] == 'Higher education', 'edu_type_labelencoding'] = 3\n",
    "    train.loc[train['edu_type'] == 'Incomplete higher', 'edu_type_labelencoding'] = 2\n",
    "    train.loc[train['edu_type'] == 'Secondary / secondary special', 'edu_type_labelencoding'] = 1\n",
    "    train.loc[train['edu_type'] == 'Lower secondary', 'edu_type_labelencoding'] = 0\n",
    "    train.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "    test['edu_type_labelencoding'] = 999\n",
    "    test.loc[test['edu_type'] == 'Academic degree', 'edu_type_labelencoding'] = 4\n",
    "    test.loc[test['edu_type'] == 'Higher education', 'edu_type_labelencoding'] = 3\n",
    "    test.loc[test['edu_type'] == 'Incomplete higher', 'edu_type_labelencoding'] = 2\n",
    "    test.loc[test['edu_type'] == 'Secondary / secondary special', 'edu_type_labelencoding'] = 1\n",
    "    test.loc[test['edu_type'] == 'Lower secondary', 'edu_type_labelencoding'] = 0\n",
    "    test.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 8. 근로변수 구간화-> 20 ~ 40세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'] * -1\n",
    "    train['DAYS_EMPLOYED_bin'] = 9999\n",
    "    train.loc[ ( (train['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    train.loc[(0 < train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    train.loc[(365*5 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    train.loc[(365*10 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    train.loc[(365*20 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    train.loc[(365*30 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    train.loc[(365*40 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    train.loc[(365*50 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    train.loc[(365*60 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    train.loc[(365*70 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "    test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'] * -1\n",
    "    test['DAYS_EMPLOYED_bin'] = 9999\n",
    "    test.loc[ ( (test['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    test.loc[(0 < test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    test.loc[(365*5 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    test.loc[(365*10 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    test.loc[(365*20 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    test.loc[(365*30 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    test.loc[(365*40 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    test.loc[(365*50 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    test.loc[(365*60 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    test.loc[(365*70 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "\n",
    "\n",
    "    # 9. 근로 일수에 따른 수입 (연간 소득을 년차 평준화해주는느낌..)\n",
    "    train['EMPLOYED_INCOME'] = 9999\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    train['EMPLOYED_INCOME'] = train['EMPLOYED_INCOME'] * train['income_total']\n",
    "\n",
    "    test['EMPLOYED_INCOME'] = 9999\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    test['EMPLOYED_INCOME'] = test['EMPLOYED_INCOME'] * test['income_total']\n",
    "\n",
    "\n",
    "\n",
    "    #11. value_counts 변수 \n",
    "    train['income_type_count'] = train['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    train['house_type_count'] = train['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "    test['income_type_count'] = test['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    test['house_type_count'] = test['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    # max, mean, min\n",
    "    ### DAYS_BIRTH_bin\n",
    "    train['averageincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    train['maxincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    train['minincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "    test['averageincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    test['maxincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    test['minincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "\n",
    "    ### DAYS_EMPLOYED_bin\n",
    "    train['averagehouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    train['maxinhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    train['mininhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "    test['averagehouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    test['maxinhouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    test['mininhouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "\n",
    "    ### house_type\n",
    "    train['averagerealhouse'] = train['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    train['maxrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    train['minrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "    test['averagerealhouse'] = test['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    test['maxrealhouse'] = test['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    test['minrealhouse'] = test['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "\n",
    "    ### edu_type_labelencoding\n",
    "    train['averageedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    train['maxedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    train['minedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "    test['averageedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    test['maxedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    test['minedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "\n",
    "    \n",
    "\n",
    "    # 그 외 열들 onehotencoding\n",
    "    OH_cols_train1 = pd.DataFrame(OH_encoder1.transform(train[['income_type']]), index=train.index, columns = list(OH_encoder1.get_feature_names()))\n",
    "    train.drop('income_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train1], axis=1)\n",
    "\n",
    "    OH_cols_test1 = pd.DataFrame(OH_encoder1.transform(test[['income_type']]), index=test.index, columns = list(OH_encoder1.get_feature_names()))\n",
    "    test.drop('income_type', axis=1, inplace=True)\n",
    "    test = pd.concat([test, OH_cols_test1], axis=1)\n",
    "\n",
    "    OH_cols_train2 = pd.DataFrame(OH_encoder2.transform(train[['house_type']]), index=train.index, columns = list(OH_encoder2.get_feature_names()))\n",
    "    train.drop('house_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train2], axis=1)\n",
    "\n",
    "    OH_cols_test2 = pd.DataFrame(OH_encoder2.transform(test[['house_type']]), index=test.index, columns = list(OH_encoder2.get_feature_names()))\n",
    "    test.drop('house_type', axis=1, inplace=True)\n",
    "    test = pd.concat([test, OH_cols_test2], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    # binary sum 열 생성\n",
    "    binary = ['gender','car','reality','work_phone','phone','email']\n",
    "    train['bin_sum'] = train[binary].sum(axis=1)\n",
    "    test['bin_sum'] = test[binary].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # StandardScaler & minmaxscaler\n",
    "    train['income_stand'] = standardscaler.transform(train[['income_total']])\n",
    "    test['income_stand'] = standardscaler.transform(test[['income_total']])\n",
    "\n",
    "    train['income_minmax'] = minmaxscaler.transform(train[['income_total']])\n",
    "    test['income_minmax'] = minmaxscaler.transform(test[['income_total']])\n",
    "\n",
    "\n",
    "\n",
    "    # KMEAN\n",
    "    train_x = train.drop([\"credit\"], axis = 1)\n",
    "\n",
    "    if mode == 'valid':\n",
    "        test_x = test.drop([\"credit\"], axis = 1)\n",
    "    elif mode == 'test':\n",
    "        test_x = test.copy()\n",
    "        \n",
    "\n",
    "    train_kmean = kmeans.transform(train_x)\n",
    "    train_kmean = pd.DataFrame(train_kmean, columns=[f\"Centroid_{i+1}\" for i in range(train_kmean.shape[1])], index=train_x.index)\n",
    "    train = pd.concat([train, train_kmean], axis=1)\n",
    "\n",
    "    test_kmean = kmeans.transform(test_x)\n",
    "    test_kmean = pd.DataFrame(test_kmean, columns=[f\"Centroid_{i+1}\" for i in range(test_kmean.shape[1])], index=test_x.index)\n",
    "    test = pd.concat([test, test_kmean], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # 일자 관련 변수\n",
    "    ### DAYS_BIRTH\n",
    "    train['DAYS_BIRTH_month']=np.floor((-train['DAYS_BIRTH'])/30)-((np.floor((-train['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "    train['DAYS_BIRTH_week']=np.floor((-train['DAYS_BIRTH'])/7)-((np.floor((-train['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "    test['DAYS_BIRTH_month']=np.floor((-test['DAYS_BIRTH'])/30)-((np.floor((-test['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "    test['DAYS_BIRTH_week']=np.floor((-test['DAYS_BIRTH'])/7)-((np.floor((-test['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "    ### DAYS_EMPLOYED\n",
    "    train['DAYS_EMPLOYED_month']=np.floor((-train['DAYS_EMPLOYED'])/30)-((np.floor((-train['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    train['DAYS_EMPLOYED_week']=np.floor((-train['DAYS_EMPLOYED'])/7)-((np.floor((-train['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "    test['DAYS_EMPLOYED_month']=np.floor((-test['DAYS_EMPLOYED'])/30)-((np.floor((-test['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    test['DAYS_EMPLOYED_week']=np.floor((-test['DAYS_EMPLOYED'])/7)-((np.floor((-test['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "    ### before_EMPLOYED\n",
    "    train['before_EMPLOYED']=train['DAYS_BIRTH']-train['DAYS_EMPLOYED']\n",
    "    train['before_EMPLOYED_month']=np.floor((-train['before_EMPLOYED'])/30)-((np.floor((-train['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    train['before_EMPLOYED_week']=np.floor((-train['before_EMPLOYED'])/7)-((np.floor((-train['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "    test['before_EMPLOYED']=test['DAYS_BIRTH']-test['DAYS_EMPLOYED']\n",
    "    test['before_EMPLOYED_month']=np.floor((-test['before_EMPLOYED'])/30)-((np.floor((-test['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    test['before_EMPLOYED_week']=np.floor((-test['before_EMPLOYED'])/7)-((np.floor((-test['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRNkrYTgkeYc"
   },
   "source": [
    "## Feature Engineering Using AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1ZNeDoKRhwQ"
   },
   "outputs": [],
   "source": [
    "# AutoEncoder 모델링 시 kmean 값들은 제외\n",
    "train_tuning, test_tuning = preprocessing(train_original2, test_original2, 'test')\n",
    "\n",
    "train_tuning = train_tuning.drop([f'Centroid_{i}' for i in range(1,11)], axis=1)\n",
    "train_tuning = train_tuning.drop('credit', axis=1)\n",
    "test_tuning = test_tuning.drop([f'Centroid_{i}' for i in range(1,11)], axis=1)\n",
    "\n",
    "train_tuning_columns = train_tuning.columns\n",
    "test_tuning_columns = test_tuning.columns\n",
    "\n",
    "# 딥러닝을 위한 정규화\n",
    "tuningdata_scaler = MinMaxScaler()\n",
    "train_tuning = tuningdata_scaler.fit_transform(train_tuning)\n",
    "train_tuning = pd.DataFrame(train_tuning, columns = train_tuning_columns)\n",
    "\n",
    "test_tuning = tuningdata_scaler.transform(test_tuning)\n",
    "test_tuning = pd.DataFrame(test_tuning, columns = test_tuning_columns)\n",
    "\n",
    "print(f'train의 shape: {train_tuning.shape}')\n",
    "print(f'test의 shape: {test_tuning.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1R8-s1FgknUb"
   },
   "outputs": [],
   "source": [
    "# 변수 전처리\n",
    "cat_cols = ['gender', 'car', 'reality', 'edu_type_labelencoding']\n",
    "num_cols = list(train_tuning.columns)\n",
    "for col in cat_cols:\n",
    "    num_cols.remove(col)\n",
    "\n",
    "print(f'카테고리 변수: {len(cat_cols)}개')\n",
    "print(f'연속형 변수: {len(num_cols)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "md1kZaiYnIUh"
   },
   "outputs": [],
   "source": [
    "# 모델링\n",
    "encoding_dim = 64\n",
    "\n",
    "def get_model(df, encoding_dim, dropout=.2):\n",
    "    num_dim = len(num_cols)\n",
    "    num_input = keras.layers.Input((num_dim,), name='num_input')\n",
    "    cat_inputs = []\n",
    "    cat_embs = []\n",
    "    emb_dims = 0\n",
    "    for col in cat_cols:\n",
    "        cat_input = keras.layers.Input((1,), name=f'{col}_input')\n",
    "        emb_dim = max(8, int(np.log2(1 + df[col].max()) * 4))\n",
    "        cat_emb = keras.layers.Embedding(input_dim=df[col].nunique() + 1, output_dim=emb_dim)(cat_input)\n",
    "        cat_emb = keras.layers.Dropout(dropout)(cat_emb)\n",
    "        cat_emb = keras.layers.Reshape((emb_dim,))(cat_emb)\n",
    "\n",
    "        cat_inputs.append(cat_input)\n",
    "        cat_embs.append(cat_emb)\n",
    "        emb_dims += emb_dim\n",
    "\n",
    "    merged_inputs = keras.layers.Concatenate()([num_input] + cat_embs)\n",
    "\n",
    "    encoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(merged_inputs)\n",
    "    encoded = keras.layers.Dropout(dropout)(encoded)\n",
    "    encoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    encoded = keras.layers.Dropout(dropout)(encoded)    \n",
    "    encoded = keras.layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    decoded = keras.layers.Dense(encoding_dim * 2, activation='relu')(encoded)\n",
    "    decoded = keras.layers.Dropout(dropout)(decoded)\n",
    "    decoded = keras.layers.Dense(encoding_dim * 3, activation='relu')(decoded)\n",
    "    decoded = keras.layers.Dropout(dropout)(decoded)    \n",
    "    decoded = keras.layers.Dense(num_dim + emb_dims, activation='linear')(encoded)\n",
    "\n",
    "    encoder = keras.Model([num_input] + cat_inputs, encoded)\n",
    "    ae = keras.Model([num_input] + cat_inputs, decoded)\n",
    "\n",
    "    ae.add_loss(keras.losses.mean_absolute_error(merged_inputs, decoded))\n",
    "    optm = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    ae.compile(optimizer=optm)\n",
    "    return ae, encoder\n",
    "\n",
    "ae, encoder = get_model(train_tuning, encoding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mGI4jfnnOiQ"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "filename = '/content/drive/MyDrive/dacon_card_predict/Autoencoder.h5'\n",
    "\n",
    "inputs = [train_tuning[num_cols].values] + [train_tuning[x].values for x in cat_cols]\n",
    "inputs_test = [test_tuning[num_cols].values] + [test_tuning[x].values for x in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\n",
    "checkpoint = ModelCheckpoint(filename,             \n",
    "                                 monitor='val_loss',   \n",
    "                                 verbose=1,            \n",
    "                                 save_best_only=True,  \n",
    "                                 mode='min'          \n",
    "                                )\n",
    "\n",
    "ae.fit(inputs, inputs,\n",
    "      epochs=100000,\n",
    "      batch_size=512,\n",
    "      shuffle=True,\n",
    "      validation_split=0.2,\n",
    "      callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "ae.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29N-ZUwwJRDh"
   },
   "outputs": [],
   "source": [
    "train, test = preprocessing(train_original3, test_original3, 'test')\n",
    "print('train의 Shape: {}'.format(train.shape))\n",
    "print('test의 Shape: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1RuAbZybDp5p"
   },
   "outputs": [],
   "source": [
    "encoding_train = encoder.predict(inputs)\n",
    "encoding_train = pd.DataFrame(encoding_train, columns=[f'enc_{x}' for x in range(encoding_dim)])\n",
    "train = pd.concat([train, encoding_train], axis=1)\n",
    "\n",
    "encoding_test = encoder.predict(inputs_test)\n",
    "encoding_test = pd.DataFrame(encoding_test, columns=[f'enc_{x}' for x in range(encoding_dim)])\n",
    "test = pd.concat([test, encoding_test], axis=1)\n",
    "\n",
    "print('train의 Shape: {}'.format(train.shape))\n",
    "print('test의 Shape: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCLETumTMbty"
   },
   "outputs": [],
   "source": [
    "modeling_col = list(train.mean()[train.mean() != 0].index)\n",
    "print(f'모델링에 활용할 변수의 개수: {len(modeling_col)}')\n",
    "\n",
    "train = train[modeling_col]\n",
    "modeling_col.remove('credit')\n",
    "test = test[modeling_col]\n",
    "\n",
    "print('train의 Shape: {}'.format(train.shape))\n",
    "print('test의 Shape: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6JSrdw9Cx4L"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "    mi_scores = mutual_info_classif(X, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "X = train.drop('credit', axis=1)\n",
    "y = train['credit']\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XZQtJE3CzT-"
   },
   "outputs": [],
   "source": [
    "mi_scores[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjdnYLNqggi"
   },
   "source": [
    "## 하이퍼파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raafO8oovKF0"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "train_tuning = train.copy()\n",
    "train_tuning_x = train_tuning.drop([\"credit\"], axis = 1)\n",
    "train_tuning_y = train_tuning['credit']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_tuning_x, train_tuning_y,\n",
    "                 stratify = train_tuning_y, \n",
    "                 test_size = 0.2,\n",
    "                 random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HoTP6rgrxmd"
   },
   "outputs": [],
   "source": [
    "# 파라미터 도출\n",
    "params = {'num_class': 3,\n",
    "          'learning_rate':0.01}\n",
    "\n",
    "\n",
    "clf = AutoLGB(objective='multiclass', metric='multi_logloss', params=params, \n",
    "                feature_selection=False, n_est=10000)\n",
    "\n",
    "clf.tune(X_train, y_train)\n",
    "n_best = clf.n_best # n_estimates 횟수\n",
    "features = clf.features # 사용된 변수\n",
    "params = clf.params # 파라미터들\n",
    "print(f'best iteration: {n_best}')\n",
    "print(f'selected features ({len(features)}): {features}') \n",
    "print(f'params: {params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCXQRgwEZYrV"
   },
   "source": [
    "## KFOLD 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shceNSt2uh8A"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "modeling_col = modeling_col + ['credit']\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train_original, train_original['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "\n",
    "lgb_models={}\n",
    "for fold in range(5):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    \n",
    "    TRAIN = train_original.iloc[train_idx].reset_index(drop=True)\n",
    "    VALID = train_original.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    TRAIN, VALID = preprocessing(TRAIN, VALID, 'valid')\n",
    "\n",
    "    # Autoencoder fitting ================================================================\n",
    "    TRAIN_ae = TRAIN.copy()\n",
    "    VALID_ae = VALID.copy()\n",
    "\n",
    "    TRAIN_ae = TRAIN_ae.drop([f'Centroid_{i}' for i in range(1,11)], axis=1)\n",
    "    TRAIN_ae = TRAIN_ae.drop('credit', axis=1)\n",
    "    VALID_ae = VALID_ae.drop([f'Centroid_{i}' for i in range(1,11)], axis=1)\n",
    "    VALID_ae = VALID_ae.drop('credit', axis=1)\n",
    "\n",
    "    TRAIN_ae_columns = TRAIN_ae.columns\n",
    "    VALID_ae_columns = VALID_ae.columns\n",
    "\n",
    "    TRAIN_ae = tuningdata_scaler.transform(TRAIN_ae)\n",
    "    TRAIN_ae = pd.DataFrame(TRAIN_ae, columns = TRAIN_ae_columns)\n",
    "\n",
    "    VALID_ae = tuningdata_scaler.transform(VALID_ae)\n",
    "    VALID_ae = pd.DataFrame(VALID_ae, columns = VALID_ae_columns)\n",
    "\n",
    "    inputs_train = [TRAIN_ae[num_cols].values] + [TRAIN_ae[x].values for x in cat_cols]\n",
    "    inputs_valid = [VALID_ae[num_cols].values] + [VALID_ae[x].values for x in cat_cols]  \n",
    "\n",
    "    encoding_train = encoder.predict(inputs_train)\n",
    "    encoding_train = pd.DataFrame(encoding_train, columns=[f'enc_{x}' for x in range(encoding_dim)])\n",
    "    encoding_valid = encoder.predict(inputs_valid)\n",
    "    encoding_valid = pd.DataFrame(encoding_valid, columns=[f'enc_{x}' for x in range(encoding_dim)])\n",
    "    # Autoencoder fitting ================================================================\n",
    "\n",
    "    TRAIN = pd.concat([TRAIN, encoding_train], axis=1)\n",
    "    TRAIN = TRAIN[modeling_col]\n",
    "    \n",
    "    VALID = pd.concat([VALID, encoding_valid], axis=1)\n",
    "    VALID = VALID[modeling_col]\n",
    "\n",
    "    X_train = TRAIN.drop(['credit'],axis=1).values\n",
    "    X_valid = VALID.drop(['credit'],axis=1).values\n",
    "    y_train = TRAIN['credit'].values\n",
    "    y_valid = VALID['credit'].values \n",
    "\n",
    "    lgb_dtrain = lgb.Dataset(data = X_train, label = y_train) \n",
    "    lgb_dvalid = lgb.Dataset(data = X_valid, label = y_valid) \n",
    "\n",
    "    lgb_model = lgb.train(params, lgb_dtrain, 10000, valid_sets=[lgb_dvalid], early_stopping_rounds=100, verbose_eval=200)\n",
    "    lgb_models[fold] = lgb_model\n",
    "\n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8Z309zAzCbr"
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dop3wDYFokU"
   },
   "outputs": [],
   "source": [
    "submission.iloc[:,1:]=0\n",
    "for fold in range(5):\n",
    "    submission.iloc[:,1:] += lgb_models[fold].predict(test)/5\n",
    "\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZmsQjD3Fq-T"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"/content/drive/MyDrive/dacon_card_predict/submission/0508.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP4zU0KBBmKDbiYiuB+rkbP",
   "collapsed_sections": [],
   "name": "0509_AutoEncoder_Loss:MAE_AutoEncoderInputScaling의 사본",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
