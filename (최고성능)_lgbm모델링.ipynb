{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EITJvSPBM4aO"
   },
   "source": [
    "# 구글 마운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcY2NsQiTfAN"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UreujEpXMxIc"
   },
   "source": [
    "# **라이브러리 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQPHkLzjMxIh"
   },
   "outputs": [],
   "source": [
    "! pip install kaggler\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import os\n",
    "from kaggler.model import AutoLGB\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, log_loss\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3Xms5aZMxIi"
   },
   "outputs": [],
   "source": [
    "print('현재경로: {}'.format(os.getcwd()))\n",
    "\n",
    "# index는 무의미한 열이며, FLAGE_MOBIL은 모두 같은 값을 지니기에 삭제\n",
    "train = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/train.csv\")\n",
    "test = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/test.csv\")\n",
    "submission = pd.read_csv(\"/content/drive/MyDrive/dacon_card_predict/data/sample_submission.csv\")\n",
    "\n",
    "\n",
    "train.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
    "test.drop(['index', 'FLAG_MOBIL'], axis=1, inplace=True)\n",
    "\n",
    "train_original = train.copy()\n",
    "train_original2 = train.copy()\n",
    "test_original = test.copy()\n",
    "test_original2 = test.copy()\n",
    "\n",
    "print('train의 Shape: {}'.format(train.shape))\n",
    "print('test의 Shape: {}'.format(test.shape))\n",
    "print('submission의 Shape: {}'.format(submission.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uHoBFmsakkf"
   },
   "source": [
    "## FIT 객체 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eov38OoRT9fN"
   },
   "outputs": [],
   "source": [
    "def make_fit_instance(train):\n",
    "\n",
    "\n",
    "    # 결측치 처리\n",
    "    train.loc[train['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    train['occyp_type'] = train['occyp_type'].fillna('None')\n",
    "\n",
    "\n",
    "    # labelencoding 열 추가\n",
    "    ## (1). occyp_type을 credit_score로 label_encoding\n",
    "    ### occyp_type 별 credit의 value_count 산정\n",
    "    cnt = 0\n",
    "    for jop in train['occyp_type'].unique():\n",
    "\n",
    "        tmp = pd.DataFrame(round(train.loc[train['occyp_type'] == jop, 'credit'].value_counts() / train.loc[train['occyp_type'] == jop, 'credit'].shape[0],3)).rename(columns={'credit':jop})\n",
    "\n",
    "        if cnt == 0:\n",
    "            concat_df = tmp.copy()\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df,tmp], axis=1)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    ### occyp_type 별 credit_score 산출\n",
    "    concat_df_t = concat_df.transpose().rename(columns={0:'credit:0',\n",
    "                                        1:'credit:1',\n",
    "                                        2:'credit:2'})\n",
    "    concat_df_t['score'] = concat_df_t['credit:0'] * 0 + concat_df_t['credit:1'] * 1 + concat_df_t['credit:2'] * 2\n",
    "\n",
    "    ### Scaling\n",
    "    mmscaler = MinMaxScaler()\n",
    "    dict_occyp_type = pd.DataFrame(mmscaler.fit_transform(concat_df_t['score'].values.reshape(-1,1)), columns=['score'], index=concat_df_t.index)['score'].to_dict()\n",
    "    train['occyp_type_labelencoding'] = train['occyp_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "\n",
    "    ### drop occyp_type\n",
    "    train.drop('occyp_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    ## (2). income_type을 credit_score로 label_encoding\n",
    "    ### income_type 별 credit의 value_count 산정\n",
    "    cnt = 0\n",
    "    for jop in train['income_type'].unique():\n",
    "\n",
    "        tmp = pd.DataFrame(round(train.loc[train['income_type'] == jop, 'credit'].value_counts() / train.loc[train['income_type'] == jop, 'credit'].shape[0],3)).rename(columns={'credit':jop})\n",
    "\n",
    "        if cnt == 0:\n",
    "            concat_df = tmp.copy()\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df,tmp], axis=1)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    ### income_type 별 credit_score 산출\n",
    "    concat_df_t = concat_df.transpose().rename(columns={0:'credit:0',\n",
    "                                        1:'credit:1',\n",
    "                                        2:'credit:2'})\n",
    "    concat_df_t['score'] = concat_df_t['credit:1'] * 1 + concat_df_t['credit:2'] * 2\n",
    "\n",
    "    ### Scaling\n",
    "    mmscaler = MinMaxScaler()\n",
    "    dict_income_type = pd.DataFrame(mmscaler.fit_transform(concat_df_t['score'].values.reshape(-1,1)), columns=['score'], index=concat_df_t.index)['score'].to_dict()\n",
    "    train['income_type_labelencoding'] = train['income_type'].apply(lambda x:dict_income_type.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    ## (3). house_type을 credit_score로 label_encoding\n",
    "    ### house_type 별 credit의 value_count 산정\n",
    "    cnt = 0\n",
    "    for jop in train['house_type'].unique():\n",
    "\n",
    "        tmp = pd.DataFrame(round(train.loc[train['house_type'] == jop, 'credit'].value_counts() / train.loc[train['house_type'] == jop, 'credit'].shape[0],3)).rename(columns={'credit':jop})\n",
    "\n",
    "        if cnt == 0:\n",
    "            concat_df = tmp.copy()\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df,tmp], axis=1)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    ### house_type 별 credit_score 산출\n",
    "    concat_df_t = concat_df.transpose().rename(columns={0:'credit:0',\n",
    "                                        1:'credit:1',\n",
    "                                        2:'credit:2'})\n",
    "    concat_df_t['score'] = concat_df_t['credit:0'] * 0 + concat_df_t['credit:1'] * 1 + concat_df_t['credit:2'] * 2\n",
    "\n",
    "    ### Scaling\n",
    "    mmscaler = MinMaxScaler()\n",
    "    dict_house_type = pd.DataFrame(mmscaler.fit_transform(concat_df_t['score'].values.reshape(-1,1)), columns=['score'], index=concat_df_t.index)['score'].to_dict()\n",
    "    train['house_type_labelencoding'] = train['house_type'].apply(lambda x:dict_house_type.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    ## (4). edu_type을 credit_score로 label_encoding\n",
    "    ### edu_type 별 credit의 value_count 산정\n",
    "    cnt = 0\n",
    "    for jop in train['edu_type'].unique():\n",
    "\n",
    "        tmp = pd.DataFrame(round(train.loc[train['edu_type'] == jop, 'credit'].value_counts() / train.loc[train['edu_type'] == jop, 'credit'].shape[0],3)).rename(columns={'credit':jop})\n",
    "\n",
    "        if cnt == 0:\n",
    "            concat_df = tmp.copy()\n",
    "        else:\n",
    "            concat_df = pd.concat([concat_df,tmp], axis=1)\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "    ### edu_type 별 credit_score 산출\n",
    "    concat_df_t = concat_df.transpose().rename(columns={0:'credit:0',\n",
    "                                        1:'credit:1',\n",
    "                                        2:'credit:2'})\n",
    "    concat_df_t['score'] = concat_df_t['credit:0'] * 0 + concat_df_t['credit:1'] * 1 + concat_df_t['credit:2'] * 2\n",
    "\n",
    "    ### Scaling\n",
    "    mmscaler = MinMaxScaler()\n",
    "    dict_edu_type = pd.DataFrame(mmscaler.fit_transform(concat_df_t['score'].values.reshape(-1,1)), columns=['score'], index=concat_df_t.index)['score'].to_dict()\n",
    "    train['edu_type_labelencoding'] = train['edu_type'].apply(lambda x:dict_edu_type.get(x,0))\n",
    "    train.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 자동차와 집은 고가 재산 --> 두개 모두 소유 vs 한개만소유 vs 아예 없는 유형 유의미할 듯?\n",
    "    train['gender'] = train['gender'].replace(['F','M'], [0,  1])\n",
    "    train['car'] = train['car'].replace(['N', 'Y'], [0, 1])\n",
    "    train['reality'] = train['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    train['car_reality'] = train['car'] + train['reality']\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 나이변수 구간화 --> 20 ~ 69세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_BIRTH'] = train['DAYS_BIRTH'] * -1\n",
    "    train['DAYS_BIRTH_bin'] = 9999\n",
    "    train.loc[(365*20 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    train.loc[(365*30 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    train.loc[(365*40 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    train.loc[(365*50 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    train.loc[(365*60 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "\n",
    "\n",
    "    # 4. 아이들의 수: 없음 // 1~2명 // 3명이상으로 구분 \n",
    "    train['child_num_group'] = 99\n",
    "    train.loc[train['child_num'] == 0, 'child_num_group'] = 0\n",
    "    train.loc[train['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    train.loc[train['child_num'] > 2, 'child_num_group'] = 2\n",
    "    train.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 가족 사이즈 1 // 2~4 // 5~ 구분\n",
    "    train['family_size_group'] = 99\n",
    "    train.loc[train['family_size'] == 1, 'family_size_group'] = 0\n",
    "    train.loc[train['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    train.loc[train['family_size'] > 4, 'family_size_group'] = 2\n",
    "    train.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 6. 결혼 // 혼자사는사람 // 결혼을 했으나 사정상 혼자사는 사람 0,1,2 구분\n",
    "    train['family_type_group'] = 999\n",
    "    train.loc[train['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    train.loc[train['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    train.loc[train['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    train.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 8. 근로변수 구간화-> 20 ~ 40세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'] * -1\n",
    "    train['DAYS_EMPLOYED_bin'] = 9999\n",
    "    train.loc[ ( (train['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    train.loc[(0 < train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    train.loc[(365*5 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    train.loc[(365*10 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    train.loc[(365*20 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    train.loc[(365*30 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    train.loc[(365*40 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    train.loc[(365*50 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    train.loc[(365*60 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    train.loc[(365*70 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "\n",
    "\n",
    "    # 9. 근로 일수에 따른 수입 (연간 소득을 년차 평준화해주는느낌..)\n",
    "    train['EMPLOYED_INCOME'] = 9999\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    train['EMPLOYED_INCOME'] = train['EMPLOYED_INCOME'] * train['income_total']\n",
    "\n",
    "\n",
    "\n",
    "    # FIT value_counts() 변수\n",
    "    dict_income_type_valuecount = train['income_type'].value_counts().to_dict()\n",
    "    dict_house_type_valuecount = train['house_type'].value_counts().to_dict()\n",
    "    train['income_type_count'] = train['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    train['house_type_count'] = train['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # max, mean, min\n",
    "    ### DAYS_BIRTH_bin\n",
    "    dict_DAYS_BIRTH_bin_mean = train.groupby('DAYS_BIRTH_bin').agg('mean')['income_total'].to_dict()\n",
    "    dict_DAYS_BIRTH_bin_max = train.groupby('DAYS_BIRTH_bin').agg('max')['income_total'].to_dict()\n",
    "    dict_DAYS_BIRTH_bin_min = train.groupby('DAYS_BIRTH_bin').agg('min')['income_total'].to_dict()\n",
    "    train['averageincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    train['maxincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    train['minincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "\n",
    "    ### DAYS_EMPLOYED_bin\n",
    "    dict_DAYS_EMPLOYED_bin_mean = train.groupby('DAYS_EMPLOYED_bin').agg('mean')['income_total'].to_dict()\n",
    "    dict_DAYS_EMPLOYED_bin_max = train.groupby('DAYS_EMPLOYED_bin').agg('max')['income_total'].to_dict()\n",
    "    dict_DAYS_EMPLOYED_bin_min = train.groupby('DAYS_EMPLOYED_bin').agg('min')['income_total'].to_dict()\n",
    "    train['averagehouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    train['maxinhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    train['mininhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "\n",
    "    ### house_type\n",
    "    dict_house_type_mean = train.groupby('house_type').agg('mean')['income_total'].to_dict()\n",
    "    dict_house_type_max = train.groupby('house_type').agg('max')['income_total'].to_dict()\n",
    "    dict_house_type_min = train.groupby('house_type').agg('min')['income_total'].to_dict()\n",
    "    train['averagerealhouse'] = train['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    train['maxrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    train['minrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "\n",
    "    ### edu_type_labelencoding\n",
    "    dict_edu_type_labelencoding_mean = train.groupby('edu_type_labelencoding').agg('mean')['income_total'].to_dict()\n",
    "    dict_edu_type_labelencoding_max = train.groupby('edu_type_labelencoding').agg('max')['income_total'].to_dict()\n",
    "    dict_edu_type_labelencoding_min = train.groupby('edu_type_labelencoding').agg('min')['income_total'].to_dict()\n",
    "    train['averageedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    train['maxedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    train['minedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "\n",
    "\n",
    "   \n",
    "    # FIT onehotencoder\n",
    "    OH_encoder1 = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_encoder1.fit_transform(train[['income_type']])\n",
    "    OH_cols_train1 = pd.DataFrame(OH_encoder1.fit_transform(train[['income_type']]), index=train.index)\n",
    "    train.drop('income_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train1], axis=1)\n",
    "\n",
    "    OH_encoder2 = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_encoder2.fit_transform(train[['house_type']])\n",
    "    OH_cols_train2 = pd.DataFrame(OH_encoder2.fit_transform(train[['house_type']]), index=train.index)\n",
    "    train.drop('house_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train2], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # binary sum 열 생성\n",
    "    binary = ['gender','car','reality','work_phone','phone','email']\n",
    "    train['bin_sum'] = train[binary].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # FIT scaler\n",
    "    standardscaler = StandardScaler()\n",
    "    train['income_stand'] = standardscaler.fit_transform(train[['income_total']])\n",
    "\n",
    "    minmaxscaler = MinMaxScaler()\n",
    "    train['income_minmax'] = minmaxscaler.fit_transform(train[['income_total']])\n",
    "\n",
    "\n",
    "\n",
    "    # KMEAN\n",
    "    train_x = train.drop([\"credit\"], axis = 1)\n",
    "    kmeans = KMeans(n_clusters=10, n_init=10, random_state=0)\n",
    "    kmeans.fit(train_x)\n",
    "\n",
    "\n",
    "\n",
    "    return dict_occyp_type, dict_income_type, dict_house_type, dict_edu_type, dict_income_type_valuecount, dict_house_type_valuecount, dict_DAYS_BIRTH_bin_mean, dict_DAYS_BIRTH_bin_max, dict_DAYS_BIRTH_bin_min, dict_DAYS_EMPLOYED_bin_mean, dict_DAYS_EMPLOYED_bin_max, dict_DAYS_EMPLOYED_bin_min, dict_house_type_mean, dict_house_type_max, dict_house_type_min, dict_edu_type_labelencoding_mean, dict_edu_type_labelencoding_max, dict_edu_type_labelencoding_min, OH_encoder1, OH_encoder2, standardscaler, minmaxscaler, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77lnH33HZqYf"
   },
   "outputs": [],
   "source": [
    "dict_occyp_type, dict_income_type, dict_house_type, dict_edu_type, dict_income_type_valuecount, dict_house_type_valuecount, dict_DAYS_BIRTH_bin_mean, dict_DAYS_BIRTH_bin_max, dict_DAYS_BIRTH_bin_min, dict_DAYS_EMPLOYED_bin_mean, dict_DAYS_EMPLOYED_bin_max, dict_DAYS_EMPLOYED_bin_min, dict_house_type_mean, dict_house_type_max, dict_house_type_min, dict_edu_type_labelencoding_mean, dict_edu_type_labelencoding_max, dict_edu_type_labelencoding_min, OH_encoder1, OH_encoder2, standardscaler, minmaxscaler, kmeans = make_fit_instance(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgzbXnbEXFhY"
   },
   "source": [
    "## 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R4cTJYXXHAh"
   },
   "outputs": [],
   "source": [
    "def preprocessing(train, test, mode):\n",
    "\n",
    "\n",
    "\n",
    "    # 결측치 처리\n",
    "    train.loc[train['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    train['occyp_type'] = train['occyp_type'].fillna('None')\n",
    "    test.loc[test['DAYS_EMPLOYED'] > 0, 'occyp_type'] = 'NoJop'\n",
    "    test['occyp_type'] = test['occyp_type'].fillna('None')\n",
    "\n",
    "\n",
    "\n",
    "    # 1. label_encoding\n",
    "    train['occyp_type_labelencoding'] = train['occyp_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "    test['occyp_type_labelencoding'] = test['occyp_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "    train.drop('occyp_type', axis=1, inplace=True)\n",
    "    test.drop('occyp_type', axis=1, inplace=True)\n",
    "\n",
    "    train['income_type_labelencoding'] = train['income_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "    test['income_type_labelencoding'] = test['income_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "\n",
    "    train['house_type_labelencoding'] = train['house_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "    test['house_type_labelencoding'] = test['house_type'].apply(lambda x:dict_occyp_type.get(x,0))\n",
    "\n",
    "    train['edu_type_labelencoding'] = train['edu_type'].apply(lambda x:dict_edu_type.get(x,0))\n",
    "    test['edu_type_labelencoding'] = test['edu_type'].apply(lambda x:dict_edu_type.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 2. 자동차와 집은 고가 재산 --> 두개 모두 소유 vs 한개만소유 vs 아예 없는 유형 유의미할 듯?\n",
    "    train['gender'] = train['gender'].replace(['F','M'], [0,  1])\n",
    "    train['car'] = train['car'].replace(['N', 'Y'], [0, 1])\n",
    "    train['reality'] = train['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    train['car_reality'] = train['car'] + train['reality']\n",
    "\n",
    "    test['gender'] = test['gender'].replace(['F','M'], [0,  1])\n",
    "    test['car'] = test['car'].replace(['N', 'Y'], [0, 1])\n",
    "    test['reality'] = test['reality'].replace(['N', 'Y'], [0, 1])\n",
    "    test['car_reality'] = test['car'] + test['reality']\n",
    "\n",
    "\n",
    "\n",
    "    # 3. 나이변수 구간화 --> 20 ~ 69세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_BIRTH'] = train['DAYS_BIRTH'] * -1\n",
    "    train['DAYS_BIRTH_bin'] = 9999\n",
    "    train.loc[(365*20 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    train.loc[(365*30 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    train.loc[(365*40 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    train.loc[(365*50 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    train.loc[(365*60 <= train['DAYS_BIRTH']) & (train['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "    test['DAYS_BIRTH'] = test['DAYS_BIRTH'] * -1\n",
    "    test['DAYS_BIRTH_bin'] = 9999\n",
    "    test.loc[(365*20 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*30), 'DAYS_BIRTH_bin'] = 1\n",
    "    test.loc[(365*30 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*40), 'DAYS_BIRTH_bin'] = 2\n",
    "    test.loc[(365*40 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*50), 'DAYS_BIRTH_bin'] = 3\n",
    "    test.loc[(365*50 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*60), 'DAYS_BIRTH_bin'] = 4\n",
    "    test.loc[(365*60 <= test['DAYS_BIRTH']) & (test['DAYS_BIRTH'] < 365*70), 'DAYS_BIRTH_bin'] = 5\n",
    "\n",
    "\n",
    "\n",
    "    # 4. 아이들의 수: 없음 // 1~2명 // 3명이상으로 구분 \n",
    "    train['child_num_group'] = 99\n",
    "    train.loc[train['child_num'] == 0, 'child_num_group'] = 0\n",
    "    train.loc[train['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    train.loc[train['child_num'] > 2, 'child_num_group'] = 2\n",
    "    train.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "    test['child_num_group'] = 99\n",
    "    test.loc[test['child_num'] == 0, 'child_num_group'] = 0\n",
    "    test.loc[test['child_num'].isin([1,2]), 'child_num_group'] = 1\n",
    "    test.loc[test['child_num'] > 2, 'child_num_group'] = 2\n",
    "    test.drop('child_num', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 5. 가족 사이즈 1 // 2~4 // 5~ 구분\n",
    "    train['family_size_group'] = 99\n",
    "    train.loc[train['family_size'] == 1, 'family_size_group'] = 0\n",
    "    train.loc[train['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    train.loc[train['family_size'] > 4, 'family_size_group'] = 2\n",
    "    train.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "    test['family_size_group'] = 99\n",
    "    test.loc[test['family_size'] == 1, 'family_size_group'] = 0\n",
    "    test.loc[test['family_size'].isin([2,3,4]), 'family_size_group'] = 1\n",
    "    test.loc[test['family_size'] > 4, 'family_size_group'] = 2\n",
    "    test.drop('family_size', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 6. 결혼 // 혼자사는사람 // 결혼을 했으나 사정상 혼자사는 사람 0,1,2 구분\n",
    "    train['family_type_group'] = 999\n",
    "    train.loc[train['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    train.loc[train['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    train.loc[train['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    train.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "    test['family_type_group'] = 999\n",
    "    test.loc[test['family_type'].isin(['Married','Civil marriage']), 'family_type_group'] = 0\n",
    "    test.loc[test['family_type'].isin(['Single / not married']), 'family_type_group'] = 1\n",
    "    test.loc[test['family_type'].isin(['Separated','Widow']), 'family_type_group'] = 2\n",
    "    test.drop('family_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 7. edu_type 학력순으로 label-encoding \n",
    "    train['edu_type_labelencoding'] = 999\n",
    "    train.loc[train['edu_type'] == 'Academic degree', 'edu_type_labelencoding'] = 4\n",
    "    train.loc[train['edu_type'] == 'Higher education', 'edu_type_labelencoding'] = 3\n",
    "    train.loc[train['edu_type'] == 'Incomplete higher', 'edu_type_labelencoding'] = 2\n",
    "    train.loc[train['edu_type'] == 'Secondary / secondary special', 'edu_type_labelencoding'] = 1\n",
    "    train.loc[train['edu_type'] == 'Lower secondary', 'edu_type_labelencoding'] = 0\n",
    "    train.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "    test['edu_type_labelencoding'] = 999\n",
    "    test.loc[test['edu_type'] == 'Academic degree', 'edu_type_labelencoding'] = 4\n",
    "    test.loc[test['edu_type'] == 'Higher education', 'edu_type_labelencoding'] = 3\n",
    "    test.loc[test['edu_type'] == 'Incomplete higher', 'edu_type_labelencoding'] = 2\n",
    "    test.loc[test['edu_type'] == 'Secondary / secondary special', 'edu_type_labelencoding'] = 1\n",
    "    test.loc[test['edu_type'] == 'Lower secondary', 'edu_type_labelencoding'] = 0\n",
    "    test.drop('edu_type', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    # 8. 근로변수 구간화-> 20 ~ 40세까지 존재 --> 20대, 30대 등,,, 으로 mapping\n",
    "    train['DAYS_EMPLOYED'] = train['DAYS_EMPLOYED'] * -1\n",
    "    train['DAYS_EMPLOYED_bin'] = 9999\n",
    "    train.loc[ ( (train['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    train.loc[(0 < train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    train.loc[(365*5 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    train.loc[(365*10 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    train.loc[(365*20 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    train.loc[(365*30 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    train.loc[(365*40 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    train.loc[(365*50 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    train.loc[(365*60 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    train.loc[(365*70 <= train['DAYS_EMPLOYED']) & (train['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "    test['DAYS_EMPLOYED'] = test['DAYS_EMPLOYED'] * -1\n",
    "    test['DAYS_EMPLOYED_bin'] = 9999\n",
    "    test.loc[ ( (test['DAYS_EMPLOYED'] < 0 )), 'DAYS_EMPLOYED_bin'] = 0 # 무직\n",
    "    test.loc[(0 < test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*5), 'DAYS_EMPLOYED_bin'] = 1 #1년차~4년차 (사회초년생)\n",
    "    test.loc[(365*5 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*10), 'DAYS_EMPLOYED_bin'] = 2 # 5년차~9년차 \n",
    "    test.loc[(365*10 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*20), 'DAYS_EMPLOYED_bin'] = 3 # 10년차~20년차\n",
    "    test.loc[(365*20 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*30), 'DAYS_EMPLOYED_bin'] = 4 # 20년차~30년차\n",
    "    test.loc[(365*30 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*40), 'DAYS_EMPLOYED_bin'] = 5 # 30년차~40년차\n",
    "    test.loc[(365*40 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*50), 'DAYS_EMPLOYED_bin'] = 6 # 40년차~50년차\n",
    "    test.loc[(365*50 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*60), 'DAYS_EMPLOYED_bin'] = 7\n",
    "    test.loc[(365*60 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*70), 'DAYS_EMPLOYED_bin'] = 8\n",
    "    test.loc[(365*70 <= test['DAYS_EMPLOYED']) & (test['DAYS_EMPLOYED'] < 365*80), 'DAYS_EMPLOYED_bin'] = 9\n",
    "\n",
    "\n",
    "\n",
    "    # 9. 근로 일수에 따른 수입 (연간 소득을 년차 평준화해주는느낌..)\n",
    "    train['EMPLOYED_INCOME'] = 9999\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    train.loc[(train.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    train['EMPLOYED_INCOME'] = train['EMPLOYED_INCOME'] * train['income_total']\n",
    "\n",
    "    test['EMPLOYED_INCOME'] = 9999\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 0),'EMPLOYED_INCOME'] = 0\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 1),'EMPLOYED_INCOME'] = 6/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 2),'EMPLOYED_INCOME'] = 5/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 3),'EMPLOYED_INCOME'] = 4/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 4),'EMPLOYED_INCOME'] = 3/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 5),'EMPLOYED_INCOME'] = 2/21\n",
    "    test.loc[(test.DAYS_EMPLOYED_bin== 6),'EMPLOYED_INCOME'] = 1/21\n",
    "    test['EMPLOYED_INCOME'] = test['EMPLOYED_INCOME'] * test['income_total']\n",
    "\n",
    "\n",
    "\n",
    "    #11. value_counts 변수 \n",
    "    train['income_type_count'] = train['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    train['house_type_count'] = train['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "    test['income_type_count'] = test['income_type'].apply(lambda x:dict_income_type_valuecount.get(x,0))\n",
    "    test['house_type_count'] = test['house_type'].apply(lambda x:dict_house_type_valuecount.get(x,0))\n",
    "\n",
    "\n",
    "\n",
    "    # max, mean, min\n",
    "    ### DAYS_BIRTH_bin\n",
    "    train['averageincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    train['maxincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    train['minincome'] = train['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "    test['averageincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_mean.get(x,0))\n",
    "    test['maxincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_max.get(x,0))\n",
    "    test['minincome'] = test['DAYS_BIRTH_bin'].apply(lambda x:dict_DAYS_BIRTH_bin_min.get(x,0))\n",
    "\n",
    "    ### DAYS_EMPLOYED_bin\n",
    "    train['averagehouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    train['maxinhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    train['mininhouse'] = train['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "    test['averagehouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_mean.get(x,0))\n",
    "    test['maxinhouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_max.get(x,0))\n",
    "    test['mininhouse'] = test['DAYS_EMPLOYED_bin'].apply(lambda x:dict_DAYS_EMPLOYED_bin_min.get(x,0))\n",
    "\n",
    "    ### house_type\n",
    "    train['averagerealhouse'] = train['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    train['maxrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    train['minrealhouse'] = train['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "    test['averagerealhouse'] = test['house_type'].apply(lambda x:dict_house_type_mean.get(x,0))\n",
    "    test['maxrealhouse'] = test['house_type'].apply(lambda x:dict_house_type_max.get(x,0))\n",
    "    test['minrealhouse'] = test['house_type'].apply(lambda x:dict_house_type_min.get(x,0))\n",
    "\n",
    "    ### edu_type_labelencoding\n",
    "    train['averageedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    train['maxedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    train['minedu'] = train['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "    test['averageedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_mean.get(x,0))\n",
    "    test['maxedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_max.get(x,0))\n",
    "    test['minedu'] = test['edu_type_labelencoding'].apply(lambda x:dict_edu_type_labelencoding_min.get(x,0))\n",
    "\n",
    "    \n",
    "\n",
    "    # 그 외 열들 onehotencoding\n",
    "    OH_cols_train1 = pd.DataFrame(OH_encoder1.transform(train[['income_type']]), index=train.index)\n",
    "    train.drop('income_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train1], axis=1)\n",
    "\n",
    "    OH_cols_test1 = pd.DataFrame(OH_encoder1.transform(test[['income_type']]), index=test.index)\n",
    "    test.drop('income_type', axis=1, inplace=True)\n",
    "    test = pd.concat([test, OH_cols_test1], axis=1)\n",
    "\n",
    "    OH_cols_train2 = pd.DataFrame(OH_encoder2.transform(train[['house_type']]), index=train.index)\n",
    "    train.drop('house_type', axis=1, inplace=True)\n",
    "    train = pd.concat([train, OH_cols_train2], axis=1)\n",
    "\n",
    "    OH_cols_test2 = pd.DataFrame(OH_encoder2.transform(test[['house_type']]), index=test.index)\n",
    "    test.drop('house_type', axis=1, inplace=True)\n",
    "    test = pd.concat([test, OH_cols_test2], axis=1)\n",
    "    \n",
    "\n",
    "\n",
    "    # binary sum 열 생성\n",
    "    binary = ['gender','car','reality','work_phone','phone','email']\n",
    "    train['bin_sum'] = train[binary].sum(axis=1)\n",
    "    test['bin_sum'] = test[binary].sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # StandardScaler & minmaxscaler\n",
    "    train['income_stand'] = standardscaler.transform(train[['income_total']])\n",
    "    test['income_stand'] = standardscaler.transform(test[['income_total']])\n",
    "\n",
    "    train['income_minmax'] = minmaxscaler.transform(train[['income_total']])\n",
    "    test['income_minmax'] = minmaxscaler.transform(test[['income_total']])\n",
    "\n",
    "\n",
    "\n",
    "    # KMEAN\n",
    "    train_x = train.drop([\"credit\"], axis = 1)\n",
    "\n",
    "    if mode == 'valid':\n",
    "        test_x = test.drop([\"credit\"], axis = 1)\n",
    "    elif mode == 'test':\n",
    "        test_x = test.copy()\n",
    "        \n",
    "\n",
    "    train_kmean = kmeans.transform(train_x)\n",
    "    train_kmean = pd.DataFrame(train_kmean, columns=[f\"Centroid_{i+1}\" for i in range(train_kmean.shape[1])], index=train_x.index)\n",
    "    train = pd.concat([train, train_kmean], axis=1)\n",
    "\n",
    "    test_kmean = kmeans.transform(test_x)\n",
    "    test_kmean = pd.DataFrame(test_kmean, columns=[f\"Centroid_{i+1}\" for i in range(test_kmean.shape[1])], index=test_x.index)\n",
    "    test = pd.concat([test, test_kmean], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # 일자 관련 변수\n",
    "    ### DAYS_BIRTH\n",
    "    train['DAYS_BIRTH_year'] = np.floor(-train['DAYS_BIRTH'] / 365)\n",
    "    train['DAYS_BIRTH_month']=np.floor((-train['DAYS_BIRTH'])/30)-((np.floor((-train['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "    train['DAYS_BIRTH_week']=np.floor((-train['DAYS_BIRTH'])/7)-((np.floor((-train['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "    test['DAYS_BIRTH_year'] = np.floor(-test['DAYS_BIRTH'] / 365)\n",
    "    test['DAYS_BIRTH_month']=np.floor((-test['DAYS_BIRTH'])/30)-((np.floor((-test['DAYS_BIRTH'])/30)/12).astype(int)*12)\n",
    "    test['DAYS_BIRTH_week']=np.floor((-test['DAYS_BIRTH'])/7)-((np.floor((-test['DAYS_BIRTH'])/7)/4).astype(int)*4)\n",
    "\n",
    "    ### DAYS_EMPLOYED\n",
    "    train['DAYS_EMPLOYED_year'] = np.floor(-train['DAYS_EMPLOYED'] / 365)\n",
    "    train['DAYS_EMPLOYED_month']=np.floor((-train['DAYS_EMPLOYED'])/30)-((np.floor((-train['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    train['DAYS_EMPLOYED_week']=np.floor((-train['DAYS_EMPLOYED'])/7)-((np.floor((-train['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "    test['DAYS_EMPLOYED_year'] = np.floor(-test['DAYS_EMPLOYED'] / 365)\n",
    "    test['DAYS_EMPLOYED_month']=np.floor((-test['DAYS_EMPLOYED'])/30)-((np.floor((-test['DAYS_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    test['DAYS_EMPLOYED_week']=np.floor((-test['DAYS_EMPLOYED'])/7)-((np.floor((-test['DAYS_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "    ### before_EMPLOYED\n",
    "    train['before_EMPLOYED']=train['DAYS_BIRTH']-train['DAYS_EMPLOYED']\n",
    "    train['before_EMPLOYED_year'] = np.floor(-train['before_EMPLOYED'] / 365)\n",
    "    train['before_EMPLOYED_month']=np.floor((-train['before_EMPLOYED'])/30)-((np.floor((-train['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    train['before_EMPLOYED_week']=np.floor((-train['before_EMPLOYED'])/7)-((np.floor((-train['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "    test['before_EMPLOYED']=test['DAYS_BIRTH']-test['DAYS_EMPLOYED']\n",
    "    test['before_EMPLOYED_year'] = np.floor(-test['before_EMPLOYED'] / 365)\n",
    "    test['before_EMPLOYED_month']=np.floor((-test['before_EMPLOYED'])/30)-((np.floor((-test['before_EMPLOYED'])/30)/12).astype(int)*12)\n",
    "    test['before_EMPLOYED_week']=np.floor((-test['before_EMPLOYED'])/7)-((np.floor((-test['before_EMPLOYED'])/7)/4).astype(int)*4)\n",
    "\n",
    "\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmjdnYLNqggi"
   },
   "source": [
    "## 하이퍼파라미터 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raafO8oovKF0"
   },
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "train_tuning, test = preprocessing(train_original2, test_original2, 'test')\n",
    "\n",
    "train_tuning_x = train_tuning.drop([\"credit\"], axis = 1)\n",
    "train_tuning_y = train_tuning['credit']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_tuning_x, train_tuning_y,\n",
    "                 stratify = train_tuning_y, \n",
    "                 test_size = 0.2,\n",
    "                 random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HoTP6rgrxmd"
   },
   "outputs": [],
   "source": [
    "# 파라미터 도출\n",
    "params = {'num_class': 3,\n",
    "          'learning_rate':0.01}\n",
    "\n",
    "\n",
    "clf = AutoLGB(objective='multiclass', metric='multi_logloss', params=params, \n",
    "                feature_selection=False, n_est=10000)\n",
    "\n",
    "clf.tune(X_train, y_train)\n",
    "n_best = clf.n_best # n_estimates 횟수\n",
    "features = clf.features # 사용된 변수\n",
    "params = clf.params # 파라미터들\n",
    "print(f'best iteration: {n_best}')\n",
    "print(f'selected features ({len(features)}): {features}') \n",
    "print(f'params: {params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCXQRgwEZYrV"
   },
   "source": [
    "## KFOLD 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shceNSt2uh8A"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "folds=[]\n",
    "for train_idx, valid_idx in skf.split(train_original, train_original['credit']):\n",
    "    folds.append((train_idx, valid_idx))\n",
    "\n",
    "lgb_models={}\n",
    "for fold in range(5):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    \n",
    "    TRAIN = train_original.iloc[train_idx]\n",
    "    VALID = train_original.iloc[valid_idx]\n",
    "\n",
    "    TRAIN, VALID = preprocessing(TRAIN, VALID, 'valid')\n",
    "\n",
    "    X_train = TRAIN.drop(['credit'],axis=1).values\n",
    "    X_valid = VALID.drop(['credit'],axis=1).values\n",
    "    y_train = TRAIN['credit'].values\n",
    "    y_valid = VALID['credit'].values \n",
    "\n",
    "    lgb_dtrain = lgb.Dataset(data = X_train, label = y_train) \n",
    "    lgb_dvalid = lgb.Dataset(data = X_valid, label = y_valid) \n",
    "\n",
    "    \n",
    "    lgb_model = lgb.train(params, lgb_dtrain, 10000, valid_sets=[lgb_dvalid], early_stopping_rounds=100, verbose_eval=200)\n",
    "    lgb_models[fold] = lgb_model\n",
    "\n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8Z309zAzCbr"
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dop3wDYFokU"
   },
   "outputs": [],
   "source": [
    "submission.iloc[:,1:]=0\n",
    "for fold in range(5):\n",
    "    submission.iloc[:,1:] += lgb_models[fold].predict(test)/5\n",
    "\n",
    "submission.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZmsQjD3Fq-T"
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"/content/drive/MyDrive/dacon_card_predict/submission/0510_3.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "0510_category type 데이터에 라벨인코딩 방안을 모두 도입",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
